{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YukiAoki-GU/IoT_for_beginners/blob/main/teachable_machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "057adb30-5f12-470d-bdff-4526a4da018a",
      "metadata": {
        "id": "057adb30-5f12-470d-bdff-4526a4da018a"
      },
      "outputs": [],
      "source": [
        "#TensorFlow Liteランタイムのインストール\n",
        "!pip3 install tflite-runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32225f11-7e20-421e-981b-a03242883b7c",
      "metadata": {
        "id": "32225f11-7e20-421e-981b-a03242883b7c"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def capture():\n",
        "    # カメラを初期化\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Camera could not be opened.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                print(\"Error: Could not read frame from camera.\")\n",
        "                continue\n",
        "\n",
        "            # 画像を表示（Jupyter内に表示）\n",
        "            clear_output(wait=True)  # 前回の画像をクリア\n",
        "            plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))  # BGRからRGBに変換\n",
        "            plt.axis('off')  # 軸を非表示\n",
        "            plt.show()\n",
        "            time.sleep(1)  # 必要に応じて間隔を調整\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Stopped by user.\")\n",
        "    finally:\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "capture()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56365bce-050d-4d57-987a-93c8df4555e5",
      "metadata": {
        "id": "56365bce-050d-4d57-987a-93c8df4555e5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tflite_runtime.interpreter as tflite\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def load_labels(path):\n",
        "    with open(path, 'r') as f:\n",
        "        return{i: line.strip() for i, line in enumerate(f.readlines())}\n",
        "\n",
        "def capture_and_predict():\n",
        "    # モデルとラベルの読み込み\n",
        "    interpreter = tflite.Interpreter(model_path=\"model.tflite\")\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    labels = load_labels(\"./labels.txt\")\n",
        "\n",
        "    # 入力テンソルのデータ型を取得\n",
        "    input_dtype = input_details[0]['dtype']\n",
        "\n",
        "    # カメラを初期化\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Camera could not be opened.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                print(\"Error: Could not read frame from camera.\")\n",
        "                continue\n",
        "\n",
        "            # 画像を表示（Jupyter内に表示）\n",
        "            clear_output(wait=True)  # 前回の画像をクリア\n",
        "            plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))  # BGRからRGBに変換\n",
        "            plt.axis('off')  # 軸を非表示\n",
        "            plt.show()\n",
        "\n",
        "            # 画像の前処理\n",
        "            img = cv2.resize(frame, (224, 224))\n",
        "\n",
        "            if input_dtype == np.uint8:\n",
        "                # UINT8に変換\n",
        "                img = img.astype(np.uint8)\n",
        "            elif input_dtype == np.float32:\n",
        "                # FLOAT32に変換して正規化\n",
        "                img = img.astype(np.float32) / 255.0\n",
        "\n",
        "            img = np.expand_dims(img, axis=0)\n",
        "\n",
        "            # 推論\n",
        "            interpreter.set_tensor(input_details[0]['index'], img)\n",
        "            interpreter.invoke()\n",
        "            output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "            # 結果の表示\n",
        "            predicted_class = np.argmax(output_data)\n",
        "            confidence = output_data[0][predicted_class]\n",
        "\n",
        "            print(f\"Predicted class: {labels[predicted_class]}, Confidence: {confidence:.4f}\")\n",
        "\n",
        "            time.sleep(1)  # 必要に応じて間隔を調整\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Stopped by user.\")\n",
        "    finally:\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "capture_and_predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "155eb831-7eb5-4514-b5a4-500710ce1d40",
      "metadata": {
        "id": "155eb831-7eb5-4514-b5a4-500710ce1d40"
      },
      "source": [
        "---\n",
        "# sound classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58c7191d-98bd-4332-aad0-848f1a965757",
      "metadata": {
        "id": "58c7191d-98bd-4332-aad0-848f1a965757"
      },
      "outputs": [],
      "source": [
        "! pip3 install sounddevice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36104bff-9126-4db4-a739-9f0950b0e1af",
      "metadata": {
        "id": "36104bff-9126-4db4-a739-9f0950b0e1af"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sounddevice as sd\n",
        "import tflite_runtime.interpreter as tflite\n",
        "import scipy.signal\n",
        "import time\n",
        "\n",
        "# モデルとラベルのパス\n",
        "MODEL_PATH = \"sound/soundclassifier_with_metadata.tflite\"\n",
        "LABELS_PATH = \"sound/labels.txt\"\n",
        "\n",
        "# サンプリングレート（Teachable Machineのモデルに合わせて調整）\n",
        "SAMPLE_RATE = 16000\n",
        "DURATION = 1  # 秒\n",
        "\n",
        "# モデルをロード\n",
        "interpreter = tflite.Interpreter(model_path=MODEL_PATH)\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# モデルの入力形状を確認\n",
        "print(f\"Model expects input shape: {input_details[0]['shape']}\")\n",
        "\n",
        "# ラベルをロード\n",
        "with open(LABELS_PATH, \"r\") as f:\n",
        "    labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# オーディオキャプチャと前処理\n",
        "def preprocess_audio(audio, sample_rate):\n",
        "    # モデルが期待する入力サイズを取得\n",
        "    expected_length = input_details[0]['shape'][1]\n",
        "    # リサンプリング\n",
        "    resampled_audio = scipy.signal.resample(audio, expected_length)\n",
        "    # ゼロパディング\n",
        "    padded_audio = np.zeros(expected_length)\n",
        "    padded_audio[:min(len(resampled_audio), len(padded_audio))] = resampled_audio\n",
        "    # 正規化と整形\n",
        "    audio_input = np.expand_dims(padded_audio, axis=0).astype(np.float32) / np.max(np.abs(padded_audio))\n",
        "    return audio_input\n",
        "\n",
        "# 推論を実行\n",
        "def predict(audio_input):\n",
        "    interpreter.set_tensor(input_details[0]['index'], audio_input)\n",
        "    interpreter.invoke()\n",
        "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "    return np.argmax(output_data), output_data[0]\n",
        "\n",
        "# リアルタイム分類\n",
        "def classify_audio():\n",
        "    print(\"Listening...\")\n",
        "    while True:\n",
        "        try:\n",
        "            # 音声をキャプチャ\n",
        "            audio = sd.rec(int(SAMPLE_RATE * DURATION), samplerate=SAMPLE_RATE, channels=1, dtype='float32')\n",
        "            sd.wait()\n",
        "\n",
        "            # 前処理\n",
        "            audio_input = preprocess_audio(audio[:, 0], SAMPLE_RATE)\n",
        "\n",
        "            # 推論\n",
        "            predicted_class, confidence = predict(audio_input)\n",
        "\n",
        "            # 結果を表示\n",
        "            print(f\"Predicted: {labels[predicted_class]} (Confidence: {confidence[predicted_class]:.2f})\")\n",
        "\n",
        "            time.sleep(0.1)  # 必要に応じて調整\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"Stopped by user.\")\n",
        "            break\n",
        "\n",
        "# 実行\n",
        "classify_audio()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c573cb4d-c3c0-434d-9cc3-146c22d68141",
      "metadata": {
        "id": "c573cb4d-c3c0-434d-9cc3-146c22d68141"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}